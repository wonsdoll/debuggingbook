{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "1"
        ]
      },
      "source": [
        "# Statistical Debugging\n",
        "\n",
        "In this chapter, we introduce _statistical debugging_ \u2013\u00a0the idea that specific events during execution could be _statistically correlated_ with failures. We start with coverage of individual lines and then proceed towards further execution features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "2"
        ]
      },
      "outputs": [],
      "source": [
        "from bookutils import YouTubeVideo\n",
        "YouTubeVideo(\"UNuso00zYiI\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "3"
        ]
      },
      "source": [
        "**Prerequisites**\n",
        "\n",
        "* You should have read the [chapter on tracing executions](Tracer.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "slideshow": {
          "slide_type": "skip"
        },
        "tags": [
          "4"
        ]
      },
      "outputs": [],
      "source": [
        "import bookutils.setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "tags": [
          "5"
        ]
      },
      "source": [
        "## Synopsis\n",
        "<!-- Automatically generated. Do not edit. -->\n",
        "\n",
        "To [use the code provided in this chapter](Importing.ipynb), write\n",
        "\n",
        "```python\n",
        ">>> from debuggingbook.StatisticalDebugger import <identifier>\n",
        "```\n",
        "\n",
        "and then make use of the following features.\n",
        "\n",
        "\n",
        "This chapter introduces classes and techniques for _statistical debugging_ \u2013\u00a0that is, correlating specific events, such as lines covered, with passing and failing outcomes.\n",
        "\n",
        "To make use of the code in this chapter, use one of the provided `StatisticalDebugger` subclasses such as `TarantulaDebugger` or `OchiaiDebugger`. \n",
        "\n",
        "Both are instantiated with a `Collector` denoting the type of events you want to correlate outcomes with. The default `CoverageCollector`, collecting line coverage.\n",
        "\n",
        "### Collecting Events from Calls\n",
        "\n",
        "To collect events from calls that are labeled manually, use\n",
        "\n",
        "```python\n",
        ">>> debugger = TarantulaDebugger()\n",
        ">>> with debugger.collect_pass():\n",
        ">>>     remove_html_markup(\"abc\")\n",
        ">>> with debugger.collect_pass():\n",
        ">>>     remove_html_markup('<b>abc</b>')\n",
        ">>> with debugger.collect_fail():\n",
        ">>>     remove_html_markup('\"abc\"')\n",
        "```\n",
        "Within each `with` block, the _first function call_ is collected and tracked for coverage. (Note that _only_ the first call is tracked.)\n",
        "\n",
        "### Collecting Events from Tests\n",
        "\n",
        "To collect events from _tests_ that use exceptions to indicate failure, use the simpler `with` form:\n",
        "\n",
        "```python\n",
        ">>> debugger = TarantulaDebugger()\n",
        ">>> with debugger:\n",
        ">>>     remove_html_markup(\"abc\")\n",
        ">>> with debugger:\n",
        ">>>     remove_html_markup('<b>abc</b>')\n",
        ">>> with debugger:\n",
        ">>>     remove_html_markup('\"abc\"')\n",
        ">>>     assert False  # raise an exception\n",
        "```\n",
        "`with` blocks that raise an exception will be classified as failing, blocks that do not will be classified as passing. Note that exceptions raised are \"swallowed\" by the debugger.\n",
        "\n",
        "### Visualizing Events as a Table\n",
        "\n",
        "After collecting events, you can print out the observed events \u2013 in this case, line numbers \u2013\u00a0in a table, showing in which runs they occurred (`X`), and with colors highlighting the suspiciousness of the event. A \"red\" event means that the event predominantly occurs in failing runs.\n",
        "\n",
        "```python\n",
        ">>> debugger.event_table(args=True, color=True)\n",
        "```\n",
        "| `remove_html_markup` | `s='abc'` | `s='<b>abc</b>'` | `s='\"abc\"'` | \n",
        "| --------------------- | ---- | ---- | ---- | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:1</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:2</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:3</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:4</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:6</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:7</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(120.0, 50.0%, 80%)\" title=\"  0%\"> remove_html_markup:8</samp> |    - |    X |    - | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\"> remove_html_markup:9</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(120.0, 50.0%, 80%)\" title=\"  0%\">remove_html_markup:10</samp> |    - |    X |    - | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\">remove_html_markup:11</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(0.0, 100.0%, 80%)\" title=\"100%\">remove_html_markup:12</samp> |    - |    - |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\">remove_html_markup:13</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\">remove_html_markup:14</samp> |    X |    X |    X | \n",
        "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\" title=\" 50%\">remove_html_markup:16</samp> |    X |    X |    X | \n",
        "\n",
        "\n",
        "### Visualizing Suspicious Code\n",
        "\n",
        "If you collected coverage with `CoverageCollector`, you can also visualize the code with similar colors, highlighting suspicious lines:\n",
        "\n",
        "```python\n",
        ">>> debugger\n",
        "```\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 1:  50%\">   1 def remove_html_markup(s):  # type: ignore</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 2:  50%\">   2     tag = False</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 3:  50%\">   3     quote = False</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 4:  50%\">   4     out = &quot;&quot;</pre>\n",
        "<pre title=\"Line 5: not executed\">   5 &nbsp;</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 6:  50%\">   6     for c in s:</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 7:  50%\">   7         if c == &#x27;&lt;&#x27; and not quote:</pre>\n",
        "<pre style=\"background-color:hsl(120.0, 50.0%, 80%)\"\n",
        "                    title=\"Line 8:   0%\">   8             tag = True</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 9:  50%\">   9         elif c == &#x27;&gt;&#x27; and not quote:</pre>\n",
        "<pre style=\"background-color:hsl(120.0, 50.0%, 80%)\"\n",
        "                    title=\"Line 10:   0%\">  10             tag = False</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 11:  50%\">  11         elif c == &#x27;&quot;&#x27; or c == &quot;&#x27;&quot; and tag:</pre>\n",
        "<pre style=\"background-color:hsl(0.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 12: 100%\">  12             quote = not quote</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 13:  50%\">  13         elif not tag:</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 14:  50%\">  14             out = out + c</pre>\n",
        "<pre title=\"Line 15: not executed\">  15 &nbsp;</pre>\n",
        "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\"\n",
        "                    title=\"Line 16:  50%\">  16     return out</pre>\n",
        "\n",
        "\n",
        "### Ranking Events\n",
        "\n",
        "The method `rank()` returns a ranked list of events, starting with the most suspicious. This is useful for automated techniques that need potential defect locations.\n",
        "\n",
        "```python\n",
        ">>> debugger.rank()\n",
        "[('remove_html_markup', 12),\n",
        " ('remove_html_markup', 1),\n",
        " ('remove_html_markup', 6),\n",
        " ('remove_html_markup', 13),\n",
        " ('remove_html_markup', 4),\n",
        " ('remove_html_markup', 11),\n",
        " ('remove_html_markup', 2),\n",
        " ('remove_html_markup', 9),\n",
        " ('remove_html_markup', 14),\n",
        " ('remove_html_markup', 16),\n",
        " ('remove_html_markup', 7),\n",
        " ('remove_html_markup', 3),\n",
        " ('remove_html_markup', 10),\n",
        " ('remove_html_markup', 8)]\n",
        "```\n",
        "### Classes and Methods\n",
        "\n",
        "Here are all classes defined in this chapter:\n",
        "\n",
        "![](PICS/StatisticalDebugger-synopsis-1.svg)\n",
        "\n",
        "![](PICS/StatisticalDebugger-synopsis-2.svg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "6"
        ]
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The idea behind _statistical debugging_ is fairly simple. We have a program that sometimes passes and sometimes fails. This outcome can be _correlated_ with events that precede it \u2013 properties of the input, properties of the execution, properties of the program state. If we, for instance, can find that \"the program always fails when Line 123 is executed, and it always passes when Line 123 is _not_ executed\", then we have a strong correlation between Line 123 being executed and failure.\n",
        "\n",
        "Such _correlation_ does not necessarily mean _causation_. For this, we would have to prove that executing Line 123 _always_ leads to failure, and that _not_ executing it does not lead to (this) failure. Also, a correlation (or even a causation) does not mean that Line 123 contains the defect \u2013 for this, we would have to show that it actually is an error. Still, correlations make excellent hints as it comes to search for failure causes \u2013 in all generality, if you let your search be guided by _events that correlate with failures_, you are more likely to find _important hints on how the failure comes to be_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": true,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "7"
        ]
      },
      "source": [
        "## Collecting Events\n",
        "\n",
        "How can we determine events that correlate with failure? We start with a general mechanism to actually _collect_ events during execution. The abstract `Collector` class provides\n",
        "\n",
        "* a `collect()` method made for collecting events, called from the `traceit()` tracer; and\n",
        "* an `events()` method made for retrieving these events.\n",
        "\n",
        "Both of these are _abstract_ and will be defined further in subclasses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "8"
        ]
      },
      "outputs": [],
      "source": [
        "from Tracer import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "9"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "from typing import Any, Callable, Optional, Type, Tuple\n",
        "from typing import Dict, Set, List, TypeVar, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "10"
        ]
      },
      "outputs": [],
      "source": [
        "from types import FrameType, TracebackType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "11"
        ]
      },
      "outputs": [],
      "source": [
        "class Collector(Tracer):\n",
        "    \"\"\"A class to record events during execution.\"\"\"\n",
        "\n",
        "    def collect(self, frame: FrameType, event: str, arg: Any) -> None:\n",
        "        \"\"\"Collecting function. To be overridden in subclasses.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def events(self) -> Set:\n",
        "        \"\"\"Return a collection of events. To be overridden in subclasses.\"\"\"\n",
        "        return set()\n",
        "\n",
        "    def traceit(self, frame: FrameType, event: str, arg: Any) -> None:\n",
        "        self.collect(frame, event, arg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "12"
        ]
      },
      "source": [
        "A `Collector` class is used like `Tracer`, using a `with` statement. Let us apply it on the buggy variant of `remove_html_markup()` from the [Introduction to Debugging](Intro_Debugging.ipynb):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "13"
        ]
      },
      "outputs": [],
      "source": [
        "def remove_html_markup(s):  # type: ignore\n",
        "    tag = False\n",
        "    quote = False\n",
        "    out = \"\"\n",
        "\n",
        "    for c in s:\n",
        "        if c == '<' and not quote:\n",
        "            tag = True\n",
        "        elif c == '>' and not quote:\n",
        "            tag = False\n",
        "        elif c == '\"' or c == \"'\" and tag:\n",
        "            quote = not quote\n",
        "        elif not tag:\n",
        "            out = out + c\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "14"
        ]
      },
      "outputs": [],
      "source": [
        "with Collector() as c:\n",
        "    out = remove_html_markup('\"abc\"')\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "15"
        ]
      },
      "source": [
        "There's not much we can do with our collector, as the `collect()` and `events()` methods are yet empty. However, we can introduce an `id()` method which returns a string identifying the collector. This string is defined from the _first function call_ encountered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "16"
        ]
      },
      "outputs": [],
      "source": [
        "Coverage = Set[Tuple[Callable, int]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "17"
        ]
      },
      "outputs": [],
      "source": [
        "class Collector(Collector):\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Constructor.\"\"\"\n",
        "        self._function: Optional[Callable] = None\n",
        "        self._args: Optional[Dict[str, Any]] = None\n",
        "        self._argstring: Optional[str] = None\n",
        "        self._exception: Optional[Type] = None\n",
        "        self.items_to_ignore: List[Union[Type, Callable]] = [self.__class__]\n",
        "\n",
        "    def traceit(self, frame: FrameType, event: str, arg: Any) -> None:\n",
        "        \"\"\"\n",
        "        Tracing function.\n",
        "        Saves the first function and calls collect().\n",
        "        \"\"\"\n",
        "        for item in self.items_to_ignore:\n",
        "            if (isinstance(item, type) and 'self' in frame.f_locals and\n",
        "                isinstance(frame.f_locals['self'], item)):\n",
        "                # Ignore this class\n",
        "                return\n",
        "            if item.__name__ == frame.f_code.co_name:\n",
        "                # Ignore this function\n",
        "                return\n",
        "\n",
        "        if self._function is None and event == 'call':\n",
        "            # Save function\n",
        "            self._function = self.create_function(frame)\n",
        "            self._args = frame.f_locals.copy()\n",
        "            self._argstring = \", \".join([f\"{var}={repr(self._args[var])}\" \n",
        "                                         for var in self._args])\n",
        "\n",
        "        self.collect(frame, event, arg)\n",
        "\n",
        "    def collect(self, frame: FrameType, event: str, arg: Any) -> None:\n",
        "        \"\"\"Collector function. To be overloaded in subclasses.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def id(self) -> str:\n",
        "        \"\"\"Return an identifier for the collector, \n",
        "        created from the first call\"\"\"\n",
        "        return f\"{self.function().__name__}({self.argstring()})\"\n",
        "\n",
        "    def function(self) -> Callable:\n",
        "        \"\"\"Return the function from the first call, as a function object\"\"\"\n",
        "        if not self._function:\n",
        "            raise ValueError(\"No call collected\")\n",
        "        return self._function\n",
        "\n",
        "    def argstring(self) -> str:\n",
        "        \"\"\"\n",
        "        Return the list of arguments from the first call,\n",
        "        as a printable string\n",
        "        \"\"\"\n",
        "        if not self._argstring:\n",
        "            raise ValueError(\"No call collected\")\n",
        "        return self._argstring\n",
        "\n",
        "    def args(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a dict of argument names and values from the first call\"\"\"\n",
        "        if not self._args:\n",
        "            raise ValueError(\"No call collected\")\n",
        "        return self._args\n",
        "\n",
        "    def exception(self) -> Optional[Type]:\n",
        "        \"\"\"Return the exception class from the first call,\n",
        "        or None if no exception was raised.\"\"\"\n",
        "        return self._exception\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Return a string representation of the collector\"\"\"\n",
        "        # We use the ID as default representation when printed\n",
        "        return self.id()\n",
        "\n",
        "    def covered_functions(self) -> Set[Callable]:\n",
        "        \"\"\"Set of covered functions. To be overloaded in subclasses.\"\"\"\n",
        "        return set()\n",
        "\n",
        "    def coverage(self) -> Coverage:\n",
        "        \"\"\"\n",
        "        Return a set (function, lineno) with locations covered.\n",
        "        To be overloaded in subclasses.\n",
        "        \"\"\"\n",
        "        return set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "18"
        ]
      },
      "source": [
        "Here's how the collector works. We use a `with` clause to collect details on a function call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "19"
        ]
      },
      "outputs": [],
      "source": [
        "with Collector() as c:\n",
        "    remove_html_markup('abc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "20"
        ]
      },
      "source": [
        "We can now retrieve details such as the function called..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "21"
        ]
      },
      "outputs": [],
      "source": [
        "c.function()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "22"
        ]
      },
      "source": [
        "... or its arguments, as a name/value dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "23"
        ]
      },
      "outputs": [],
      "source": [
        "c.args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "24"
        ]
      },
      "source": [
        "The `id()` method returns a printable representation of the call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "25"
        ]
      },
      "outputs": [],
      "source": [
        "c.id()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "26"
        ]
      },
      "source": [
        "The `argstring()` method does the same for the argument string only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "27"
        ]
      },
      "outputs": [],
      "source": [
        "c.argstring()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "28"
        ]
      },
      "source": [
        "With this, we can collect the basic information to identify calls \u2013\u00a0such that we can later correlate their events with success or failure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "29"
        ]
      },
      "source": [
        "### Error Prevention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "30"
        ]
      },
      "source": [
        "While collecting, we'd like to avoid collecting events in the collection infrastructure. The `items_to_ignore` attribute takes care of this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "31"
        ]
      },
      "outputs": [],
      "source": [
        "class Collector(Collector):\n",
        "    def add_items_to_ignore(self,\n",
        "                            items_to_ignore: List[Union[Type, Callable]]) \\\n",
        "                            -> None:\n",
        "        \"\"\"\n",
        "        Define additional classes and functions to ignore during collection\n",
        "        (typically `Debugger` classes using these collectors).\n",
        "        \"\"\"\n",
        "        self.items_to_ignore += items_to_ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "32"
        ]
      },
      "source": [
        "If we exit a block without having collected anything, that's likely an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "33"
        ]
      },
      "outputs": [],
      "source": [
        "class Collector(Collector):\n",
        "    def __exit__(self, exc_tp: Type, exc_value: BaseException,\n",
        "                 exc_traceback: TracebackType) -> Optional[bool]:\n",
        "        \"\"\"Exit the `with` block.\"\"\"\n",
        "        ret = super().__exit__(exc_tp, exc_value, exc_traceback)\n",
        "\n",
        "        if not self._function:\n",
        "            if exc_tp:\n",
        "                return False  # re-raise exception\n",
        "            else:\n",
        "                raise ValueError(\"No call collected\")\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "34"
        ]
      },
      "source": [
        "## Collecting Coverage\n",
        "\n",
        "So far, our `Collector` class does not collect any events. Let us extend it such that it collects _coverage_ information \u2013 that is, the set of locations executed. To this end, we introduce a `CoverageCollector` subclass which saves the coverage in a set containing functions and line numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "35"
        ]
      },
      "outputs": [],
      "source": [
        "from types import FrameType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "36"
        ]
      },
      "outputs": [],
      "source": [
        "from StackInspector import StackInspector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "37"
        ]
      },
      "outputs": [],
      "source": [
        "class CoverageCollector(Collector, StackInspector):\n",
        "    \"\"\"A class to record covered locations during execution.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Constructor.\"\"\"\n",
        "        super().__init__()\n",
        "        self._coverage: Coverage = set()\n",
        "\n",
        "    def collect(self, frame: FrameType, event: str, arg: Any) -> None:\n",
        "        \"\"\"\n",
        "        Save coverage for an observed event.\n",
        "        \"\"\"\n",
        "        name = frame.f_code.co_name\n",
        "        function = self.search_func(name, frame)\n",
        "\n",
        "        if function is None:\n",
        "            function = self.create_function(frame)\n",
        "\n",
        "        location = (function, frame.f_lineno)\n",
        "        self._coverage.add(location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "38"
        ]
      },
      "source": [
        "We also override `events()` such that it returns the set of covered locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "39"
        ]
      },
      "outputs": [],
      "source": [
        "class CoverageCollector(CoverageCollector):\n",
        "    def events(self) -> Set[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Return the set of locations covered.\n",
        "        Each location comes as a pair (`function_name`, `lineno`).\n",
        "        \"\"\"\n",
        "        return {(func.__name__, lineno) for func, lineno in self._coverage}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "40"
        ]
      },
      "source": [
        "The methods `coverage()` and `covered_functions()` allow precise access to the coverage obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "41"
        ]
      },
      "outputs": [],
      "source": [
        "class CoverageCollector(CoverageCollector):\n",
        "    def covered_functions(self) -> Set[Callable]:\n",
        "        \"\"\"Return a set with all functions covered.\"\"\"\n",
        "        return {func for func, lineno in self._coverage}\n",
        "\n",
        "    def coverage(self) -> Coverage:\n",
        "        \"\"\"Return a set (function, lineno) with all locations covered.\"\"\"\n",
        "        return self._coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "42"
        ]
      },
      "source": [
        "Here is how we can use `CoverageCollector` to determine the lines executed during a run of `remove_html_markup()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "43"
        ]
      },
      "outputs": [],
      "source": [
        "with CoverageCollector() as c:\n",
        "    remove_html_markup('abc')\n",
        "c.events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "44"
        ]
      },
      "source": [
        "Sets of line numbers alone are not too revealing. They provide more insights if we actually list the code, highlighting these numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "45"
        ]
      },
      "outputs": [],
      "source": [
        "import inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "46"
        ]
      },
      "outputs": [],
      "source": [
        "from bookutils import getsourcelines    # like inspect.getsourcelines(), but in color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "47"
        ]
      },
      "outputs": [],
      "source": [
        "def code_with_coverage(function: Callable, coverage: Coverage) -> None:\n",
        "    source_lines, starting_line_number = \\\n",
        "       getsourcelines(function)\n",
        "\n",
        "    line_number = starting_line_number\n",
        "    for line in source_lines:\n",
        "        marker = '*' if (function, line_number) in coverage else ' '\n",
        "        print(f\"{line_number:4} {marker} {line}\", end='')\n",
        "        line_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "48"
        ]
      },
      "outputs": [],
      "source": [
        "code_with_coverage(remove_html_markup, c.coverage())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "49"
        ]
      },
      "source": [
        "Remember that the input `s` was `\"abc\"`? In this listing, we can see which lines were covered and which lines were not. From the listing already, we can see that `s` has neither tags nor quotes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "50"
        ]
      },
      "source": [
        "Such coverage computation plays a big role in _testing_, as one wants tests to cover as many different aspects of program execution (and notably code) as possible. But also during debugging, code coverage is essential: If some code was not even executed in the failing run, then any change to it will have no effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "51"
        ]
      },
      "outputs": [],
      "source": [
        "from bookutils import quiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "52"
        ]
      },
      "outputs": [],
      "source": [
        "quiz('Let the input be `\"<b>Don\\'t do this!</b>\"`. '\n",
        "     \"Which of these lines are executed? Use the code to find out!\",\n",
        "     [\n",
        "         \"`tag = True`\",\n",
        "         \"`tag = False`\",\n",
        "         \"`quote = not quote`\",\n",
        "         \"`out = out + c`\"\n",
        "     ], \"[ord(c) - ord('a') - 1 for c in 'cdf']\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "53"
        ]
      },
      "source": [
        "To find the solution, try this out yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "54"
        ]
      },
      "outputs": [],
      "source": [
        "with CoverageCollector() as c:\n",
        "    remove_html_markup(\"<b>Don't do this!</b>\")\n",
        "# code_with_coverage(remove_html_markup, c.coverage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "55"
        ]
      },
      "source": [
        "## Computing Differences\n",
        "\n",
        "Let us get back to the idea that we want to _correlate_ events with passing and failing outcomes. For this, we need to examine events in both _passing_ and _failing_ runs, and determine their _differences_ \u2013 since it is these differences we want to associate with their respective outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "56"
        ]
      },
      "source": [
        "### A Base Class for Statistical Debugging\n",
        "\n",
        "The `StatisticalDebugger` base class takes a collector class (such as `CoverageCollector`). Its `collect()` method creates a new collector of that very class, which will be maintained by the debugger. As argument, `collect()` takes a string characterizing the outcome (such as `'PASS'` or `'FAIL'`). This is how one would use it:\n",
        "\n",
        "```python\n",
        "debugger = StatisticalDebugger()\n",
        "with debugger.collect('PASS'):\n",
        "    some_passing_run()\n",
        "with debugger.collect('PASS'):\n",
        "    another_passing_run()\n",
        "with debugger.collect('FAIL'):\n",
        "    some_failing_run()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "57"
        ]
      },
      "source": [
        "Let us implement `StatisticalDebugger`. The base class gets a collector class as argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "58"
        ]
      },
      "outputs": [],
      "source": [
        "class StatisticalDebugger:\n",
        "    \"\"\"A class to collect events for multiple outcomes.\"\"\"\n",
        "\n",
        "    def __init__(self, collector_class: Type = CoverageCollector, log: bool = False):\n",
        "        \"\"\"Constructor. Use instances of `collector_class` to collect events.\"\"\"\n",
        "        self.collector_class = collector_class\n",
        "        self.collectors: Dict[str, List[Collector]] = {}\n",
        "        self.log = log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "59"
        ]
      },
      "source": [
        "The `collect()` method creates (and stores) a collector for the given outcome, using the given outcome to characterize the run. Any additional arguments are passed to the collector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "60"
        ]
      },
      "outputs": [],
      "source": [
        "class StatisticalDebugger(StatisticalDebugger):\n",
        "    def collect(self, outcome: str, *args: Any, **kwargs: Any) -> Collector:\n",
        "        \"\"\"Return a collector for the given outcome. \n",
        "        Additional args are passed to the collector.\"\"\"\n",
        "        collector = self.collector_class(*args, **kwargs)\n",
        "        collector.add_items_to_ignore([self.__class__])\n",
        "        return self.add_collector(outcome, collector)\n",
        "\n",
        "    def add_collector(self, outcome: str, collector: Collector) -> Collector:\n",
        "        if outcome not in self.collectors:\n",
        "            self.collectors[outcome] = []\n",
        "        self.collectors[outcome].append(collector)\n",
        "        return collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "61"
        ]
      },
      "source": [
        "The `all_events()` method produces a union of all events observed. If an outcome is given, it produces a union of all events with that outcome:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "62"
        ]
      },
      "outputs": [],
      "source": [
        "class StatisticalDebugger(StatisticalDebugger):\n",
        "    def all_events(self, outcome: Optional[str] = None) -> Set[Any]:\n",
        "        \"\"\"Return a set of all events observed.\"\"\"\n",
        "        all_events = set()\n",
        "\n",
        "        if outcome:\n",
        "            if outcome in self.collectors:\n",
        "                for collector in self.collectors[outcome]:\n",
        "                    all_events.update(collector.events())\n",
        "        else:\n",
        "            for outcome in self.collectors:\n",
        "                for collector in self.collectors[outcome]:\n",
        "                    all_events.update(collector.events())\n",
        "\n",
        "        return all_events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "63"
        ]
      },
      "source": [
        "Here's a simple example of `StatisticalDebugger` in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "64"
        ]
      },
      "outputs": [],
      "source": [
        "s = StatisticalDebugger()\n",
        "with s.collect('PASS'):\n",
        "    remove_html_markup(\"abc\")\n",
        "with s.collect('PASS'):\n",
        "    remove_html_markup('<b>abc</b>')\n",
        "with s.collect('FAIL'):\n",
        "    remove_html_markup('\"abc\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "65"
        ]
      },
      "source": [
        "The method `all_events()` returns all events collected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "66"
        ]
      },
      "outputs": [],
      "source": [
        "s.all_events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "67"
        ]
      },
      "source": [
        "If given an outcome as argument, we obtain all events with the given outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "68"
        ]
      },
      "outputs": [],
      "source": [
        "s.all_events('FAIL')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "69"
        ]
      },
      "source": [
        "The attribute `collectors` maps outcomes to lists of collectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "70"
        ]
      },
      "outputs": [],
      "source": [
        "s.collectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "71"
        ]
      },
      "source": [
        "Here's the collector of the one (and first) passing run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "72"
        ]
      },
      "outputs": [],
      "source": [
        "s.collectors['PASS'][0].id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "73"
        ]
      },
      "outputs": [],
      "source": [
        "s.collectors['PASS'][0].events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "74"
        ]
      },
      "source": [
        "To better highlight the differences between the collected events, we introduce a method `event_table()` that prints out whether an event took place in a run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "75"
        ]
      },
      "source": [
        "### Excursion: Printing an Event Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "76"
        ]
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "77"
        ]
      },
      "outputs": [],
      "source": [
        "import html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "78"
        ]
      },
      "outputs": [],
      "source": [
        "class StatisticalDebugger(StatisticalDebugger):\n",
        "    def function(self) -> Optional[Callable]:\n",
        "        \"\"\"\n",
        "        Return the entry function from the events observed,\n",
        "        or None if ambiguous.\n",
        "        \"\"\"\n",
        "        names_seen = set()\n",
        "        functions = []\n",
        "        for outcome in self.collectors:\n",
        "            for collector in self.collectors[outcome]:\n",
        "                # We may have multiple copies of the function,\n",
        "                # but sharing the same name\n",
        "                func = collector.function()\n",
        "                if func.__name__ not in names_seen:\n",
        "                    functions.append(func)\n",
        "                    names_seen.add(func.__name__)\n",
        "\n",
        "        if len(functions) != 1:\n",
        "            return None  # ambiguous\n",
        "        return functions[0]\n",
        "\n",
        "    def covered_functions(self) -> Set[Callable]:\n",
        "        \"\"\"Return a set of all functions observed.\"\"\"\n",
        "        functions = set()\n",
        "        for outcome in self.collectors:\n",
        "            for collector in self.collectors[outcome]:\n",
        "                functions |= collector.covered_functions()\n",
        "        return functions\n",
        "\n",
        "    def coverage(self) -> Coverage:\n",
        "        \"\"\"Return a set of all (functions, line_numbers) observed\"\"\"\n",
        "        coverage = set()\n",
        "        for outcome in self.collectors:\n",
        "            for collector in self.collectors[outcome]:\n",
        "                coverage |= collector.coverage()\n",
        "        return coverage\n",
        "\n",
        "    def color(self, event: Any) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Return a color for the given event, or None.\n",
        "        To be overloaded in subclasses.\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def tooltip(self, event: Any) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Return a tooltip string for the given event, or None.\n",
        "        To be overloaded in subclasses.\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def event_str(self, event: Any) -> str:\n",
        "        \"\"\"Format the given event. To be overloaded in subclasses.\"\"\"\n",
        "        if isinstance(event, str):\n",
        "            return event\n",
        "        if isinstance(event, tuple):\n",
        "            return \":\".join(self.event_str(elem) for elem in event)\n",
        "        return str(event)\n",
        "\n",
        "    def event_table_text(self, *, args: bool = False, color: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Print out a table of events observed.\n",
        "        If `args` is True, use arguments as headers.\n",
        "        If `color` is True, use colors.\n",
        "        \"\"\"\n",
        "        sep = ' | '\n",
        "        all_events = self.all_events()\n",
        "        longest_event = max(len(f\"{self.event_str(event)}\") \n",
        "                            for event in all_events)\n",
        "        out = \"\"\n",
        "\n",
        "        # Header\n",
        "        if args:\n",
        "            out += '| '\n",
        "            func = self.function()\n",
        "            if func:\n",
        "                out += '`' + func.__name__ + '`'\n",
        "            out += sep\n",
        "            for name in self.collectors:\n",
        "                for collector in self.collectors[name]:\n",
        "                    out += '`' + collector.argstring() + '`' + sep\n",
        "            out += '\\n'\n",
        "        else:\n",
        "            out += '| ' + ' ' * longest_event + sep\n",
        "            for name in self.collectors:\n",
        "                for i in range(len(self.collectors[name])):\n",
        "                    out += name + sep\n",
        "            out += '\\n'\n",
        "\n",
        "        out += '| ' + '-' * longest_event + sep\n",
        "        for name in self.collectors:\n",
        "            for i in range(len(self.collectors[name])):\n",
        "                out += '-' * len(name) + sep\n",
        "        out += '\\n'\n",
        "\n",
        "        # Data\n",
        "        for event in sorted(all_events):\n",
        "            event_name = self.event_str(event).rjust(longest_event)\n",
        "\n",
        "            tooltip = self.tooltip(event)\n",
        "            if tooltip:\n",
        "                title = f' title=\"{tooltip}\"'\n",
        "            else:\n",
        "                title = ''\n",
        "\n",
        "            if color:\n",
        "                color_name = self.color(event)\n",
        "                if color_name:\n",
        "                    event_name = \\\n",
        "                        f'<samp style=\"background-color: {color_name}\"{title}>' \\\n",
        "                        f'{html.escape(event_name)}' \\\n",
        "                        f'</samp>'\n",
        "\n",
        "            out += f\"| {event_name}\" + sep\n",
        "            for name in self.collectors:\n",
        "                for collector in self.collectors[name]:\n",
        "                    out += ' ' * (len(name) - 1)\n",
        "                    if event in collector.events():\n",
        "                        out += \"X\"\n",
        "                    else:\n",
        "                        out += \"-\"\n",
        "                    out += sep\n",
        "            out += '\\n'\n",
        "\n",
        "        return out\n",
        "\n",
        "    def event_table(self, **_args: Any) -> Any:\n",
        "        \"\"\"Print out event table in Markdown format.\"\"\"\n",
        "        return Markdown(self.event_table_text(**_args))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.event_table_text()\n",
        "\n",
        "    def _repr_markdown_(self) -> str:\n",
        "        return self.event_table_text(args=True, color=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "79"
        ]
      },
      "source": [
        "### End of Excursion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "80"
        ]
      },
      "outputs": [],
      "source": [
        "s = StatisticalDebugger()\n",
        "with s.collect('PASS'):\n",
        "    remove_html_markup(\"abc\")\n",
        "with s.collect('PASS'):\n",
        "    remove_html_markup('<b>abc</b>')\n",
        "with s.collect('FAIL'):\n",
        "    remove_html_markup('\"abc\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "81"
        ]
      },
      "outputs": [],
      "source": [
        "s.event_table(args=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "82"
        ]
      },
      "outputs": [],
      "source": [
        "quiz(\"How many lines are executed in the failing run only?\",\n",
        "     [\n",
        "         \"One\",\n",
        "         \"Two\",\n",
        "         \"Three\"\n",
        "     ], 'len([12])')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "83"
        ]
      },
      "source": [
        "Indeed, Line 12 executed in the failing run only would be a correlation to look for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "84"
        ]
      },
      "source": [
        "### Collecting Passing and Failing Runs\n",
        "\n",
        "While our `StatisticalDebugger` class allows arbitrary outcomes, we are typically only interested in two outcomes, namely _passing_ vs. _failing_ runs. We therefore introduce a specialized `DifferenceDebugger` class that provides customized methods to collect and access passing and failing runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "85"
        ]
      },
      "outputs": [],
      "source": [
        "class DifferenceDebugger(StatisticalDebugger):\n",
        "    \"\"\"A class to collect events for passing and failing outcomes.\"\"\"\n",
        "\n",
        "    PASS = 'PASS'\n",
        "    FAIL = 'FAIL'\n",
        "\n",
        "    def collect_pass(self, *args: Any, **kwargs: Any) -> Collector:\n",
        "        \"\"\"Return a collector for passing runs.\"\"\"\n",
        "        return self.collect(self.PASS, *args, **kwargs)\n",
        "\n",
        "    def collect_fail(self, *args: Any, **kwargs: Any) -> Collector:\n",
        "        \"\"\"Return a collector for failing runs.\"\"\"\n",
        "        return self.collect(self.FAIL, *args, **kwargs)\n",
        "\n",
        "    def pass_collectors(self) -> List[Collector]:\n",
        "        return self.collectors[self.PASS]\n",
        "\n",
        "    def fail_collectors(self) -> List[Collector]:\n",
        "        return self.collectors[self.FAIL]\n",
        "\n",
        "    def all_fail_events(self) -> Set[Any]:\n",
        "        \"\"\"Return all events observed in failing runs.\"\"\"\n",
        "        return self.all_events(self.FAIL)\n",
        "\n",
        "    def all_pass_events(self) -> Set[Any]:\n",
        "        \"\"\"Return all events observed in passing runs.\"\"\"\n",
        "        return self.all_events(self.PASS)\n",
        "\n",
        "    def only_fail_events(self) -> Set[Any]:\n",
        "        \"\"\"Return all events observed only in failing runs.\"\"\"\n",
        "        return self.all_fail_events() - self.all_pass_events()\n",
        "\n",
        "    def only_pass_events(self) -> Set[Any]:\n",
        "        \"\"\"Return all events observed only in passing runs.\"\"\"\n",
        "        return self.all_pass_events() - self.all_fail_events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "86"
        ]
      },
      "source": [
        "We can use `DifferenceDebugger` just as a `StatisticalDebugger`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "87"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "T1 = TypeVar('T1', bound='DifferenceDebugger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "88"
        ]
      },
      "outputs": [],
      "source": [
        "def test_debugger_html_simple(debugger: T1) -> T1:\n",
        "    with debugger.collect_pass():\n",
        "        remove_html_markup('abc')\n",
        "    with debugger.collect_pass():\n",
        "        remove_html_markup('<b>abc</b>')\n",
        "    with debugger.collect_fail():\n",
        "        remove_html_markup('\"abc\"')\n",
        "    return debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "89"
        ]
      },
      "source": [
        "However, since the outcome of tests may not always be predetermined, we provide a simpler interface for tests that can fail (= raise an exception) or pass (not raise an exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "90"
        ]
      },
      "outputs": [],
      "source": [
        "class DifferenceDebugger(DifferenceDebugger):\n",
        "    def __enter__(self) -> Any:\n",
        "        \"\"\"Enter a `with` block. Collect coverage and outcome;\n",
        "        classify as FAIL if the block raises an exception,\n",
        "        and PASS if it does not.\n",
        "        \"\"\"\n",
        "        self.collector = self.collector_class()\n",
        "        self.collector.add_items_to_ignore([self.__class__])\n",
        "        self.collector.__enter__()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_tp: Type, exc_value: BaseException,\n",
        "                 exc_traceback: TracebackType) -> Optional[bool]:\n",
        "        \"\"\"Exit the `with` block.\"\"\"\n",
        "        status = self.collector.__exit__(exc_tp, exc_value, exc_traceback)\n",
        "\n",
        "        if status is None:\n",
        "            pass\n",
        "        else:\n",
        "            return False  # Internal error; re-raise exception\n",
        "\n",
        "        if exc_tp is None:\n",
        "            outcome = self.PASS\n",
        "        else:\n",
        "            outcome = self.FAIL\n",
        "\n",
        "        self.add_collector(outcome, self.collector)\n",
        "        return True  # Ignore exception, if any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "91"
        ]
      },
      "source": [
        "Using this interface, we can rewrite `test_debugger_html()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "92"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "T2 = TypeVar('T2', bound='DifferenceDebugger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "93"
        ]
      },
      "outputs": [],
      "source": [
        "def test_debugger_html(debugger: T2) -> T2:\n",
        "    with debugger:\n",
        "        remove_html_markup('abc')\n",
        "    with debugger:\n",
        "        remove_html_markup('<b>abc</b>')\n",
        "    with debugger:\n",
        "        remove_html_markup('\"abc\"')\n",
        "        assert False  # Mark test as failing\n",
        "\n",
        "    return debugger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "94"
        ]
      },
      "outputs": [],
      "source": [
        "test_debugger_html(DifferenceDebugger())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "95"
        ]
      },
      "source": [
        "### Analyzing Events\n",
        "\n",
        "Let us now focus on _analyzing_ events collected. Since events come back as _sets_, we can compute _unions_ and _differences_ between these sets. For instance, we can compute which lines were executed in _any_ of the passing runs of `test_debugger_html()`, above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "96"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(DifferenceDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "97"
        ]
      },
      "outputs": [],
      "source": [
        "pass_1_events = debugger.pass_collectors()[0].events()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "98"
        ]
      },
      "outputs": [],
      "source": [
        "pass_2_events = debugger.pass_collectors()[1].events()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "99"
        ]
      },
      "outputs": [],
      "source": [
        "in_any_pass = pass_1_events | pass_2_events\n",
        "in_any_pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "100"
        ]
      },
      "source": [
        "Likewise, we can determine which lines were _only_ executed in the failing run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "101"
        ]
      },
      "outputs": [],
      "source": [
        "fail_events = debugger.fail_collectors()[0].events()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "102"
        ]
      },
      "outputs": [],
      "source": [
        "only_in_fail = fail_events - in_any_pass\n",
        "only_in_fail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "103"
        ]
      },
      "source": [
        "And we see that the \"failing\" run is characterized by processing quotes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "104"
        ]
      },
      "outputs": [],
      "source": [
        "code_with_coverage(remove_html_markup, only_in_fail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "105"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(DifferenceDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "106"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.all_events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "107"
        ]
      },
      "source": [
        "These are the lines executed only in the failing run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "108"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.only_fail_events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "109"
        ]
      },
      "source": [
        "These are the lines executed only in the passing runs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "110"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.only_pass_events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "111"
        ]
      },
      "source": [
        "Again, having these lines individually is neat, but things become much more interesting if we can see the associated code lines just as well. That's what we will do in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "112"
        ]
      },
      "source": [
        "## Visualizing Differences\n",
        "\n",
        "To show correlations of line coverage in context, we introduce a number of _visualization_ techniques that _highlight_ code with different colors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "113"
        ]
      },
      "source": [
        "### Discrete Spectrum\n",
        "\n",
        "The first idea is to use a _discrete_ spectrum of three colors:\n",
        "\n",
        "* _red_ for code executed in failing runs only\n",
        "* _green_ for code executed in passing runs only\n",
        "* _yellow_ for code executed in both passing and failing runs.\n",
        "\n",
        "Code that is not executed stays unhighlighted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "114"
        ]
      },
      "source": [
        "We first introduce an abstract class `SpectrumDebugger` that provides the essential functions. `suspiciousness()` returns a value between 0 and 1 indicating the suspiciousness of the given event - or `None` if unknown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "115"
        ]
      },
      "outputs": [],
      "source": [
        "class SpectrumDebugger(DifferenceDebugger):\n",
        "    def suspiciousness(self, event: Any) -> Optional[float]:\n",
        "        \"\"\"\n",
        "        Return a suspiciousness value in the range [0, 1.0]\n",
        "        for the given event, or `None` if unknown.\n",
        "        To be overloaded in subclasses.\n",
        "        \"\"\"\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "116"
        ]
      },
      "source": [
        "The `tooltip()` and `percentage()` methods convert the suspiciousness into a human-readable form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "117"
        ]
      },
      "outputs": [],
      "source": [
        "class SpectrumDebugger(SpectrumDebugger):\n",
        "    def tooltip(self, event: Any) -> str:\n",
        "        \"\"\"\n",
        "        Return a tooltip for the given event (default: percentage).\n",
        "        To be overloaded in subclasses.\n",
        "        \"\"\"\n",
        "        return self.percentage(event)\n",
        "\n",
        "    def percentage(self, event: Any) -> str:\n",
        "        \"\"\"\n",
        "        Return the suspiciousness for the given event as percentage string.\n",
        "        \"\"\"\n",
        "        suspiciousness = self.suspiciousness(event)\n",
        "        if suspiciousness is not None:\n",
        "            return str(int(suspiciousness * 100)).rjust(3) + '%'\n",
        "        else:\n",
        "            return ' ' * len('100%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "118"
        ]
      },
      "source": [
        "The `code()` method takes a function and shows each of its source code lines using the given spectrum, using HTML markup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "119"
        ]
      },
      "outputs": [],
      "source": [
        "class SpectrumDebugger(SpectrumDebugger):\n",
        "    def code(self, functions: Optional[Set[Callable]] = None, *, \n",
        "             color: bool = False, suspiciousness: bool = False,\n",
        "             line_numbers: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Return a listing of `functions` (default: covered functions).\n",
        "        If `color` is True, render as HTML, using suspiciousness colors.\n",
        "        If `suspiciousness` is True, include suspiciousness values.\n",
        "        If `line_numbers` is True (default), include line numbers.\n",
        "        \"\"\"\n",
        "\n",
        "        if not functions:\n",
        "            functions = self.covered_functions()\n",
        "\n",
        "        out = \"\"\n",
        "        seen = set()\n",
        "        for function in functions:\n",
        "            source_lines, starting_line_number = \\\n",
        "               inspect.getsourcelines(function)\n",
        "\n",
        "            if (function.__name__, starting_line_number) in seen:\n",
        "                continue\n",
        "            seen.add((function.__name__, starting_line_number))\n",
        "\n",
        "            if out:\n",
        "                out += '\\n'\n",
        "                if color:\n",
        "                    out += '<p/>'\n",
        "\n",
        "            line_number = starting_line_number\n",
        "            for line in source_lines:\n",
        "                if color:\n",
        "                    line = html.escape(line)\n",
        "                    if line.strip() == '':\n",
        "                        line = '&nbsp;'\n",
        "\n",
        "                location = (function.__name__, line_number)\n",
        "                location_suspiciousness = self.suspiciousness(location)\n",
        "                if location_suspiciousness is not None:\n",
        "                    tooltip = f\"Line {line_number}: {self.tooltip(location)}\"\n",
        "                else:\n",
        "                    tooltip = f\"Line {line_number}: not executed\"\n",
        "\n",
        "                if suspiciousness:\n",
        "                    line = self.percentage(location) + ' ' + line\n",
        "\n",
        "                if line_numbers:\n",
        "                    line = str(line_number).rjust(4) + ' ' + line\n",
        "\n",
        "                line_color = self.color(location)\n",
        "\n",
        "                if color and line_color:\n",
        "                    line = f'''<pre style=\"background-color:{line_color}\"\n",
        "                    title=\"{tooltip}\">{line.rstrip()}</pre>'''\n",
        "                elif color:\n",
        "                    line = f'<pre title=\"{tooltip}\">{line}</pre>'\n",
        "                else:\n",
        "                    line = line.rstrip()\n",
        "\n",
        "                out += line + '\\n'\n",
        "                line_number += 1\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "120"
        ]
      },
      "source": [
        "We introduce a few helper methods to visualize the code with colors in various forms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "121"
        ]
      },
      "outputs": [],
      "source": [
        "class SpectrumDebugger(SpectrumDebugger):\n",
        "    def _repr_html_(self) -> str:\n",
        "        \"\"\"When output in Jupyter, visualize as HTML\"\"\"\n",
        "        return self.code(color=True)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"Show code as string\"\"\"\n",
        "        return self.code(color=False, suspiciousness=True)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"Show code as string\"\"\"\n",
        "        return self.code(color=False, suspiciousness=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "122"
        ]
      },
      "source": [
        "So far, however, central methods like `suspiciousness()` or `color()` were abstract \u2013\u00a0that is, to be defined in subclasses. Our `DiscreteSpectrumDebugger` subclass provides concrete implementations for these, with `color()` returning one of the three colors depending on the line number:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "123"
        ]
      },
      "outputs": [],
      "source": [
        "class DiscreteSpectrumDebugger(SpectrumDebugger):\n",
        "    \"\"\"Visualize differences between executions using three discrete colors\"\"\"\n",
        "\n",
        "    def suspiciousness(self, event: Any) -> Optional[float]:\n",
        "        \"\"\"\n",
        "        Return a suspiciousness value [0, 1.0]\n",
        "        for the given event, or `None` if unknown.\n",
        "        \"\"\"\n",
        "        passing = self.all_pass_events()\n",
        "        failing = self.all_fail_events()\n",
        "\n",
        "        if event in passing and event in failing:\n",
        "            return 0.5\n",
        "        elif event in failing:\n",
        "            return 1.0\n",
        "        elif event in passing:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def color(self, event: Any) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Return a HTML color for the given event.\n",
        "        \"\"\"\n",
        "        suspiciousness = self.suspiciousness(event)\n",
        "        if suspiciousness is None:\n",
        "            return None\n",
        "\n",
        "        if suspiciousness > 0.8:\n",
        "            return 'mistyrose'\n",
        "        if suspiciousness >= 0.5:\n",
        "            return 'lightyellow'\n",
        "\n",
        "        return 'honeydew'\n",
        "\n",
        "    def tooltip(self, event: Any) -> str:\n",
        "        \"\"\"Return a tooltip for the given event.\"\"\"\n",
        "        passing = self.all_pass_events()\n",
        "        failing = self.all_fail_events()\n",
        "\n",
        "        if event in passing and event in failing:\n",
        "            return \"in passing and failing runs\"\n",
        "        elif event in failing:\n",
        "            return \"only in failing runs\"\n",
        "        elif event in passing:\n",
        "            return \"only in passing runs\"\n",
        "        else:\n",
        "            return \"never\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "124"
        ]
      },
      "source": [
        "This is how the `only_pass_events()` and `only_fail_events()` sets look like when visualized with code. The \"culprit\" line is well highlighted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "125"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(DiscreteSpectrumDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "126"
        ]
      },
      "outputs": [],
      "source": [
        "debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "127"
        ]
      },
      "source": [
        "We can clearly see that the failure is correlated with the presence of quotes in the input string (which is an important hint!). But does this also show us _immediately_ where the defect to be fixed is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "128"
        ]
      },
      "outputs": [],
      "source": [
        "quiz(\"Does the line `quote = not quote` actually contain the defect?\",\n",
        "    [\n",
        "        \"Yes, it should be fixed\",\n",
        "        \"No, the defect is elsewhere\"\n",
        "    ], '164 * 2 % 326')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "129"
        ]
      },
      "source": [
        "Indeed, it is the _governing condition_ that is wrong \u2013\u00a0that is, the condition that caused Line 12 to be executed in the first place. In order to fix a program, we have to find a location that\n",
        "\n",
        "1. _causes_ the failure (i.e., it can be changed to make the failure go away); and\n",
        "2. is a _defect_ (i.e., contains an error).\n",
        "\n",
        "In our example above, the highlighted code line is a _symptom_ for the error. To some extent, it is also a _cause_, since, say, commenting it out would also resolve the given failure, at the cost of causing other failures. However, the preceding condition also is a cause, as is the presence of quotes in the input.\n",
        "\n",
        "Only one of these also is a _defect_, though, and that is the preceding condition. Hence, while correlations can provide important hints, they do not necessarily locate defects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "130"
        ]
      },
      "source": [
        "For those of us who may not have color HTML output ready, simply printing the debugger lists suspiciousness values as percentages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "131"
        ]
      },
      "outputs": [],
      "source": [
        "print(debugger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "132"
        ]
      },
      "source": [
        "### Continuous Spectrum\n",
        "\n",
        "The criterion that an event should _only_ occur in failing runs (and not in passing runs) can be too aggressive. In particular, if we have another run that executes the \"culprit\" lines, but does _not_ fail, our \"only in fail\" criterion will no longer be helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "133"
        ]
      },
      "source": [
        "Here is an example. The input\n",
        "\n",
        "```html\n",
        "<b color=\"blue\">text</b>\n",
        "```\n",
        "\n",
        "will trigger the \"culprit\" line\n",
        "\n",
        "```python\n",
        "quote = not quote\n",
        "```\n",
        "\n",
        "but actually produce an output where the tags are properly stripped:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "134"
        ]
      },
      "outputs": [],
      "source": [
        "remove_html_markup('<b color=\"blue\">text</b>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "135"
        ]
      },
      "source": [
        "As a consequence, we no longer have lines that are being executed only in failing runs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "136"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(DiscreteSpectrumDebugger())\n",
        "with debugger.collect_pass():\n",
        "    remove_html_markup('<b link=\"blue\"></b>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "137"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.only_fail_events()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "138"
        ]
      },
      "source": [
        "In our spectrum output, the effect now is that the \"culprit\" line is as yellow as all others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "139"
        ]
      },
      "outputs": [],
      "source": [
        "debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "140"
        ]
      },
      "source": [
        "We therefore introduce a different method for highlighting lines, based on their _relative_ occurrence with respect to all runs: If a line has been _mostly_ executed in failing runs, its color should shift towards red; if a line has been _mostly_ executed in passing runs, its color should shift towards green."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "141"
        ]
      },
      "source": [
        "This _continuous spectrum_ has been introduced by the seminal _Tarantula_ tool \\cite{Jones2002}. In Tarantula, the color _hue_ for each line is defined as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "142"
        ]
      },
      "source": [
        "$$\n",
        "\\textit{color hue}(\\textit{line}) = \\textit{low color(red)} + \\frac{\\%\\textit{passed}(\\textit{line})}{\\%\\textit{passed}(\\textit{line}) + \\%\\textit{failed}(\\textit{line})} \\times \\textit{color range}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "143"
        ]
      },
      "source": [
        "Here, `%passed` and `%failed` denote the percentage at which a line has been executed in passing and failing runs, respectively. A hue of 0.0 stands for red, a hue of 1.0 stands for green, and a hue of 0.5 stands for equal fractions of red and green, yielding yellow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "144"
        ]
      },
      "source": [
        "We can implement these measures right away as methods in a new `ContinuousSpectrumDebugger` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "145"
        ]
      },
      "outputs": [],
      "source": [
        "class ContinuousSpectrumDebugger(DiscreteSpectrumDebugger):\n",
        "    \"\"\"Visualize differences between executions using a color spectrum\"\"\"\n",
        "\n",
        "    def collectors_with_event(self, event: Any, category: str) -> Set[Collector]:\n",
        "        \"\"\"\n",
        "        Return all collectors in a category\n",
        "        that observed the given event.\n",
        "        \"\"\"\n",
        "        all_runs = self.collectors[category]\n",
        "        collectors_with_event = set(collector for collector in all_runs \n",
        "                                    if event in collector.events())\n",
        "        return collectors_with_event\n",
        "\n",
        "    def collectors_without_event(self, event: Any, category: str) -> Set[Collector]:\n",
        "        \"\"\"\n",
        "        Return all collectors in a category\n",
        "        that did not observe the given event.\n",
        "        \"\"\"\n",
        "        all_runs = self.collectors[category]\n",
        "        collectors_without_event = set(collector for collector in all_runs \n",
        "                              if event not in collector.events())\n",
        "        return collectors_without_event\n",
        "\n",
        "    def event_fraction(self, event: Any, category: str) -> float:\n",
        "        if category not in self.collectors:\n",
        "            return 0.0\n",
        "\n",
        "        all_collectors = self.collectors[category]\n",
        "        collectors_with_event = self.collectors_with_event(event, category)\n",
        "        fraction = len(collectors_with_event) / len(all_collectors)\n",
        "        # print(f\"%{category}({event}) = {fraction}\")\n",
        "        return fraction\n",
        "\n",
        "    def passed_fraction(self, event: Any) -> float:\n",
        "        return self.event_fraction(event, self.PASS)\n",
        "\n",
        "    def failed_fraction(self, event: Any) -> float:\n",
        "        return self.event_fraction(event, self.FAIL)\n",
        "\n",
        "    def hue(self, event: Any) -> Optional[float]:\n",
        "        \"\"\"Return a color hue from 0.0 (red) to 1.0 (green).\"\"\"\n",
        "        passed = self.passed_fraction(event)\n",
        "        failed = self.failed_fraction(event)\n",
        "        if passed + failed > 0:\n",
        "            return passed / (passed + failed)\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "146"
        ]
      },
      "source": [
        "Having a continuous hue also implies a continuous suspiciousness and associated tooltips:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "147"
        ]
      },
      "outputs": [],
      "source": [
        "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
        "    def suspiciousness(self, event: Any) -> Optional[float]:\n",
        "        hue = self.hue(event)\n",
        "        if hue is None:\n",
        "            return None\n",
        "        return 1 - hue\n",
        "\n",
        "    def tooltip(self, event: Any) -> str:\n",
        "        return self.percentage(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "148"
        ]
      },
      "source": [
        "The hue for lines executed only in failing runs is (deep) red, as expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "149"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ContinuousSpectrumDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "150"
        ]
      },
      "outputs": [],
      "source": [
        "for location in debugger.only_fail_events():\n",
        "    print(location, debugger.hue(location))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "151"
        ]
      },
      "source": [
        "Likewise, the hue for lines executed in passing runs is (deep) green:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "152"
        ]
      },
      "outputs": [],
      "source": [
        "for location in debugger.only_pass_events():\n",
        "    print(location, debugger.hue(location))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "153"
        ]
      },
      "source": [
        "The Tarantula tool not only sets the hue for a line, but also uses _brightness_ as measure for support \u2013\u00a0that is, how often was the line executed at all. The brighter a line, the stronger the correlation with a passing or failing outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "154"
        ]
      },
      "source": [
        "The brightness is defined as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "155"
        ]
      },
      "source": [
        "$$\\textit{brightness}(line) = \\max(\\%\\textit{passed}(\\textit{line}), \\%\\textit{failed}(\\textit{line}))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "156"
        ]
      },
      "source": [
        "and it is easily implemented, too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "157"
        ]
      },
      "outputs": [],
      "source": [
        "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
        "    def brightness(self, event: Any) -> float:\n",
        "        return max(self.passed_fraction(event), self.failed_fraction(event))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "158"
        ]
      },
      "source": [
        "Our single \"only in fail\" line has a brightness of 1.0 (the maximum)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "159"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ContinuousSpectrumDebugger())\n",
        "for location in debugger.only_fail_events():\n",
        "    print(location, debugger.brightness(location))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "160"
        ]
      },
      "source": [
        "With this, we can now define a color for each line. To this end, we override the (previously discrete) `color()` method such that it returns a color specification giving hue and brightness. We use the HTML format `hsl(hue, saturation, lightness)` where the hue is given as a value between 0 and 360 (0 is red, 120 is green) and saturation and lightness are provided as percentages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "161"
        ]
      },
      "outputs": [],
      "source": [
        "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
        "    def color(self, event: Any) -> Optional[str]:\n",
        "        hue = self.hue(event)\n",
        "        if hue is None:\n",
        "            return None\n",
        "        saturation = self.brightness(event)\n",
        "\n",
        "        # HSL color values are specified with: \n",
        "        # hsl(hue, saturation, lightness).\n",
        "        return f\"hsl({hue * 120}, {saturation * 100}%, 80%)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "162"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ContinuousSpectrumDebugger())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "163"
        ]
      },
      "source": [
        "Lines executed only in failing runs are still shown in red:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "164"
        ]
      },
      "outputs": [],
      "source": [
        "for location in debugger.only_fail_events():\n",
        "    print(location, debugger.color(location))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "165"
        ]
      },
      "source": [
        "... whereas lines executed only in passing runs are still shown in green:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "166"
        ]
      },
      "outputs": [],
      "source": [
        "for location in debugger.only_pass_events():\n",
        "    print(location, debugger.color(location))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "167"
        ]
      },
      "outputs": [],
      "source": [
        "debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "168"
        ]
      },
      "source": [
        "What happens with our `quote = not quote` \"culprit\" line if it is executed in passing runs, too?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "169"
        ]
      },
      "outputs": [],
      "source": [
        "with debugger.collect_pass():\n",
        "    out = remove_html_markup('<b link=\"blue\"></b>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "170"
        ]
      },
      "outputs": [],
      "source": [
        "quiz('In which color will the `quote = not quote` \"culprit\" line '\n",
        "     'be shown after executing the above code?',\n",
        "    [\n",
        "        '<span style=\"background-color: hsl(120.0, 50.0%, 80%)\">Green</span>',\n",
        "        '<span style=\"background-color: hsl(60.0, 100.0%, 80%)\">Yellow</span>',\n",
        "        '<span style=\"background-color: hsl(30.0, 100.0%, 80%)\">Orange</span>',\n",
        "        '<span style=\"background-color: hsl(0.0, 100.0%, 80%)\">Red</span>'\n",
        "    ], '999 // 333')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "171"
        ]
      },
      "source": [
        "We see that it still is shown with an orange-red tint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "172"
        ]
      },
      "outputs": [],
      "source": [
        "debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "173"
        ]
      },
      "source": [
        "Here's another example, coming right from the Tarantula paper. The `middle()` function takes three numbers `x`, `y`, and `z`, and returns the one that is neither the minimum nor the maximum of the three:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "174"
        ]
      },
      "outputs": [],
      "source": [
        "def middle(x, y, z):  # type: ignore\n",
        "    if y < z:\n",
        "        if x < y:\n",
        "            return y\n",
        "        elif x < z:\n",
        "            return y\n",
        "    else:\n",
        "        if x > y:\n",
        "            return y\n",
        "        elif x > z:\n",
        "            return x\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "175"
        ]
      },
      "outputs": [],
      "source": [
        "middle(1, 2, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "176"
        ]
      },
      "source": [
        "Unfortunately, `middle()` can fail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "177"
        ]
      },
      "outputs": [],
      "source": [
        "middle(2, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "178"
        ]
      },
      "source": [
        "Let is see whether we can find the bug with a few additional test cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "179"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "T3 = TypeVar('T3', bound='DifferenceDebugger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "180"
        ]
      },
      "outputs": [],
      "source": [
        "def test_debugger_middle(debugger: T3) -> T3:\n",
        "    with debugger.collect_pass():\n",
        "        middle(3, 3, 5)\n",
        "    with debugger.collect_pass():\n",
        "        middle(1, 2, 3)\n",
        "    with debugger.collect_pass():\n",
        "        middle(3, 2, 1)\n",
        "    with debugger.collect_pass():\n",
        "        middle(5, 5, 5)\n",
        "    with debugger.collect_pass():\n",
        "        middle(5, 3, 4)\n",
        "    with debugger.collect_fail():\n",
        "        middle(2, 1, 3)\n",
        "    return debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "181"
        ]
      },
      "source": [
        "Note that in order to collect data from multiple function invocations, you need to have a separate `with` clause for every invocation. The following will _not_ work correctly:\n",
        "\n",
        "```python\n",
        "    with debugger.collect_pass():\n",
        "        middle(3, 3, 5)\n",
        "        middle(1, 2, 3)\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "182"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_middle(ContinuousSpectrumDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "183"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.event_table(args=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "184"
        ]
      },
      "source": [
        "Here comes the visualization. We see that the `return y` line is the culprit here \u2013\u00a0and actually also the one to be fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "185"
        ]
      },
      "outputs": [],
      "source": [
        "debugger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "186"
        ]
      },
      "outputs": [],
      "source": [
        "quiz(\"Which of the above lines should be fixed?\",\n",
        "    [\n",
        "        '<span style=\"background-color: hsl(45.0, 100%, 80%)\">Line 3: `if x < y`</span>',\n",
        "        '<span style=\"background-color: hsl(34.28571428571429, 100.0%, 80%)\">Line 5: `elif x < z`</span>',\n",
        "        '<span style=\"background-color: hsl(20.000000000000004, 100.0%, 80%)\">Line 6: `return y`</span>',\n",
        "        '<span style=\"background-color: hsl(120.0, 20.0%, 80%)\">Line 9: `return y`</span>',\n",
        "    ], r'len(\" middle  \".strip()[:3])')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "187"
        ]
      },
      "source": [
        "Indeed, in the `middle()` example, the \"reddest\" line is also the one to be fixed.  Here is the fixed version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "188"
        ]
      },
      "outputs": [],
      "source": [
        "def middle_fixed(x, y, z):  # type: ignore\n",
        "    if y < z:\n",
        "        if x < y:\n",
        "            return y\n",
        "        elif x < z:\n",
        "            return x\n",
        "    else:\n",
        "        if x > y:\n",
        "            return y\n",
        "        elif x > z:\n",
        "            return x\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "189"
        ]
      },
      "outputs": [],
      "source": [
        "middle_fixed(2, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "190"
        ]
      },
      "source": [
        "## Ranking Lines by Suspiciousness\n",
        "\n",
        "In a large program, there can be several locations (and events) that could be flagged as suspicious. It suffices that some large code block of say, 1,000 lines, is mostly executed in failing runs, and then all of this code block will be visualized in some shade of red. \n",
        "\n",
        "To further highlight the \"most suspicious\" events, one idea is to use a _ranking_ \u2013\u00a0that is, coming up with a list of events where those events most correlated with failures would be shown at the top. The programmer would then examine these events one by one and proceed down the list. We will show how this works for two \"correlation\" metrics \u2013\u00a0first the _Tarantula_ metric, as introduced above, and then the _Ochiai_ metric, which has shown to be one of the best \"ranking\" metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "191"
        ]
      },
      "source": [
        "We introduce a base class `RankingDebugger` with an abstract method `suspiciousness()` to be overloaded in subclasses. The method `rank()` returns a list of all events observed, sorted by suspiciousness, highest first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "192"
        ]
      },
      "outputs": [],
      "source": [
        "class RankingDebugger(DiscreteSpectrumDebugger):\n",
        "    \"\"\"Rank events by their suspiciousness\"\"\"\n",
        "\n",
        "    def rank(self) -> List[Any]:\n",
        "        \"\"\"Return a list of events, sorted by suspiciousness, highest first.\"\"\"\n",
        "\n",
        "        def susp(event: Any) -> float:\n",
        "            suspiciousness = self.suspiciousness(event)\n",
        "            assert suspiciousness is not None\n",
        "            return suspiciousness\n",
        "\n",
        "        events = list(self.all_events())\n",
        "        events.sort(key=susp, reverse=True)\n",
        "        return events\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return repr(self.rank())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "193"
        ]
      },
      "source": [
        "### The Tarantula Metric\n",
        "\n",
        "We can use the Tarantula metric to sort lines according to their suspiciousness. The \"redder\" a line (a hue of 0.0), the more suspicious it is. We can simply define"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "194"
        ]
      },
      "source": [
        "$$\n",
        "\\textit{suspiciousness}_\\textit{tarantula}(\\textit{event}) = 1 - \\textit{color hue}(\\textit{event})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "195"
        ]
      },
      "source": [
        "where $\\textit{color hue}$ is as defined above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "196"
        ]
      },
      "source": [
        "This is exactly the `suspiciousness()` function as already implemented in our `ContinuousSpectrumDebugger`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "197"
        ]
      },
      "source": [
        "We introduce the `TarantulaDebugger` class, inheriting visualization capabilities from the `ContinuousSpectrumDebugger` class as well as the suspiciousness features from the `RankingDebugger` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "198"
        ]
      },
      "outputs": [],
      "source": [
        "class TarantulaDebugger(ContinuousSpectrumDebugger, RankingDebugger):\n",
        "    \"\"\"Spectrum-based Debugger using the Tarantula metric for suspiciousness\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "199"
        ]
      },
      "source": [
        "Let us list `remove_html_markup()` with highlighted lines again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "200"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_html = test_debugger_html(TarantulaDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "201"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "202"
        ]
      },
      "source": [
        "Here's our ranking of lines, from most suspicious to least suspicious:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "203"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_html.rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "204"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_html.suspiciousness(tarantula_html.rank()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "205"
        ]
      },
      "source": [
        "We see that the first line in the list is indeed the most suspicious; the two \"green\" lines come at the very end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "206"
        ]
      },
      "source": [
        "For the `middle()` function, we also obtain a ranking from \"reddest\" to \"greenest\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "207"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_middle = test_debugger_middle(TarantulaDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "208"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_middle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "209"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_middle.rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "210"
        ]
      },
      "outputs": [],
      "source": [
        "tarantula_middle.suspiciousness(tarantula_middle.rank()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "211"
        ]
      },
      "source": [
        "### The Ochiai Metric\n",
        "\n",
        "The _Ochiai_ Metric \\cite{Ochiai1957} first introduced in the biology domain \\cite{daSilvaMeyer2004} and later applied for fault localization by Abreu et al. \\cite{Abreu2009}, is defined as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "212"
        ]
      },
      "source": [
        "$$\n",
        "\\textit{suspiciousness}_\\textit{ochiai} = \\frac\n",
        "{\\textit{failed}(\\textit{event})}\n",
        "{\\sqrt{\n",
        "\\bigl(\\textit{failed}(\\textit{event}) + \\textit{not-in-failed}(\\textit{event})\\bigr)\n",
        "\\times\n",
        "\\bigl(\\textit{failed}(\\textit{event}) + \\textit{passed}(\\textit{event})\\bigr)\n",
        "}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "213"
        ]
      },
      "source": [
        "where\n",
        "\n",
        "* $\\textit{failed}(\\textit{event})$ is the number of times the event occurred in _failing_ runs\n",
        "* $\\textit{not-in-failed}(\\textit{event})$ is the number of times the event did _not_ occur in failing runs\n",
        "* $\\textit{passed}(\\textit{event})$ is the number of times the event occurred in _passing_ runs.\n",
        "\n",
        "We can easily implement this formula:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "214"
        ]
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "215"
        ]
      },
      "outputs": [],
      "source": [
        "class OchiaiDebugger(ContinuousSpectrumDebugger, RankingDebugger):\n",
        "    \"\"\"Spectrum-based Debugger using the Ochiai metric for suspiciousness\"\"\"\n",
        "\n",
        "    def suspiciousness(self, event: Any) -> Optional[float]:\n",
        "        failed = len(self.collectors_with_event(event, self.FAIL))\n",
        "        not_in_failed = len(self.collectors_without_event(event, self.FAIL))\n",
        "        passed = len(self.collectors_with_event(event, self.PASS))\n",
        "\n",
        "        try:\n",
        "            return failed / math.sqrt((failed + not_in_failed) * (failed + passed))\n",
        "        except ZeroDivisionError:\n",
        "            return None\n",
        "\n",
        "    def hue(self, event: Any) -> Optional[float]:\n",
        "        suspiciousness = self.suspiciousness(event)\n",
        "        if suspiciousness is None:\n",
        "            return None\n",
        "        return 1 - suspiciousness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "216"
        ]
      },
      "source": [
        "Applied on the `remove_html_markup()` function, the individual suspiciousness scores differ from Tarantula. However, we obtain a very similar visualization, and the same ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "217"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_html = test_debugger_html(OchiaiDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "218"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "219"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_html.rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "220"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_html.suspiciousness(ochiai_html.rank()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "221"
        ]
      },
      "source": [
        "The same observations also apply for the `middle()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "222"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_middle = test_debugger_middle(OchiaiDebugger())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "223"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_middle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "224"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_middle.rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "225"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_middle.suspiciousness(ochiai_middle.rank()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "226"
        ]
      },
      "source": [
        "### How Useful is Ranking?\n",
        "\n",
        "So, which metric is better? The standard method to evaluate such rankings is to determine a _ground truth_ \u2013 that is, the set of locations that eventually are fixed \u2013 and to check at which point in the ranking any such location occurs \u2013 the earlier, the better. In our `remove_html_markup()` and `middle()` examples, both the Tarantula and the Ochiai metric perform flawlessly, as the \"culprit\" line is always ranked at the top. However, this need not always be the case; the exact performance depends on the nature of the code and the observed runs. (Also, the question of whether there always is exactly one possible location where the program can be fixed is open for discussion.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "227"
        ]
      },
      "source": [
        "You will be surprised that over time, _several dozen_ metrics have been proposed \\cite{Wong2016}, each performing somewhat better or somewhat worse depending on which benchmark they were applied on. The two metrics discussed above each have their merits \u2013 the Tarantula metric was among the first such metrics, and the Ochiai metric is generally shown to be among the most effective ones \\cite{Abreu2009}."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "228"
        ]
      },
      "source": [
        "While rankings can be easily _evaluated_, it is not necessarily clear whether and how much they serve programmers. As stated above, the assumption of rankings is that developers examine one potentially defective statement after another until they find the actually defective one. However, in a series of human studies with developers, Parnin and Orso \\cite{Parnin2011} found that this assumption may not hold:\n",
        "\n",
        "> It is unclear whether developers can actually determine the faulty nature of a statement by simply looking at it, without any additional information (e.g., the state of the program when the statement was executed or the statements that were executed before or after that one).\n",
        "\n",
        "In their study, they found that rankings could help completing a task faster, but this effect was limited to experienced developers and simpler code. Artificially changing the rank of faulty statements had little to no effect, implying that developers would not strictly follow the ranked list of statements, but rather search through the code to understand it. At this point, a _visualization_ as in the Tarantula tool can be helpful to programmers as it _guides_ the search, but a _ranking_ that _defines_ where to search may be less useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "229"
        ]
      },
      "source": [
        "Having said that, ranking has its merits \u2013\u00a0notably as it comes to informing _automated_ debugging techniques. In the [chapter on program repair](Repairer.ipynb), we will see how ranked lists of potentially faulty statements tell automated repair techniques where to try to repair the program first. And once such a repair is successful, we have a very strong indication on where and how the program could be fixed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "230"
        ]
      },
      "source": [
        "## Using Large Test Suites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "231"
        ]
      },
      "source": [
        "In fault localization, the larger and the more thorough the test suite, the higher the precision. Let us try out what happens if we extend the `middle()` test suite with additional test cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "232"
        ]
      },
      "source": [
        "The function `middle_testcase()` returns a random input for `middle()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "233"
        ]
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "234"
        ]
      },
      "outputs": [],
      "source": [
        "def middle_testcase() -> Tuple[int, int, int]:\n",
        "    x = random.randrange(10)\n",
        "    y = random.randrange(10)\n",
        "    z = random.randrange(10)\n",
        "    return x, y, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "235"
        ]
      },
      "outputs": [],
      "source": [
        "[middle_testcase() for i in range(5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "236"
        ]
      },
      "source": [
        "The function `middle_test()` simply checks if `middle()` operates correctly \u2013 by placing `x`, `y`, and `z` in a list, sorting it, and checking the middle argument. If `middle()` fails, `middle_test()` raises an exception."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "237"
        ]
      },
      "outputs": [],
      "source": [
        "def middle_test(x: int, y: int, z: int) -> None:\n",
        "    m = middle(x, y, z)\n",
        "    assert m == sorted([x, y, z])[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "238"
        ]
      },
      "outputs": [],
      "source": [
        "middle_test(4, 5, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "239"
        ]
      },
      "outputs": [],
      "source": [
        "from ExpectError import ExpectError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "240"
        ]
      },
      "outputs": [],
      "source": [
        "with ExpectError():\n",
        "    middle_test(2, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "241"
        ]
      },
      "source": [
        "The function `middle_passing_testcase()` searches and returns a triple `x`, `y`, `z` that causes `middle_test()` to pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "242"
        ]
      },
      "outputs": [],
      "source": [
        "def middle_passing_testcase() -> Tuple[int, int, int]:\n",
        "    while True:\n",
        "        try:\n",
        "            x, y, z = middle_testcase()\n",
        "            middle_test(x, y, z)\n",
        "            return x, y, z\n",
        "        except AssertionError:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "243"
        ]
      },
      "outputs": [],
      "source": [
        "(x, y, z) = middle_passing_testcase()\n",
        "m = middle(x, y, z)\n",
        "print(f\"middle({x}, {y}, {z}) = {m}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "244"
        ]
      },
      "source": [
        "The function `middle_failing_testcase()` does the same; but its triple `x`, `y`, `z` causes `middle_test()` to fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "245"
        ]
      },
      "outputs": [],
      "source": [
        "def middle_failing_testcase() -> Tuple[int, int, int]:\n",
        "    while True:\n",
        "        try:\n",
        "            x, y, z = middle_testcase()\n",
        "            middle_test(x, y, z)\n",
        "        except AssertionError:\n",
        "            return x, y, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "246"
        ]
      },
      "outputs": [],
      "source": [
        "(x, y, z) = middle_failing_testcase()\n",
        "m = middle(x, y, z)\n",
        "print(f\"middle({x}, {y}, {z}) = {m}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "247"
        ]
      },
      "source": [
        "With these, we can define two sets of test cases, each with 100 inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "248"
        ]
      },
      "outputs": [],
      "source": [
        "MIDDLE_TESTS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "249"
        ]
      },
      "outputs": [],
      "source": [
        "MIDDLE_PASSING_TESTCASES = [middle_passing_testcase()\n",
        "                            for i in range(MIDDLE_TESTS)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "250"
        ]
      },
      "outputs": [],
      "source": [
        "MIDDLE_FAILING_TESTCASES = [middle_failing_testcase()\n",
        "                            for i in range(MIDDLE_TESTS)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "251"
        ]
      },
      "source": [
        "Let us run the `OchiaiDebugger` with these two test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "252"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_middle = OchiaiDebugger()\n",
        "\n",
        "for x, y, z in MIDDLE_PASSING_TESTCASES:\n",
        "    with ochiai_middle.collect_pass():\n",
        "        middle(x, y, z)\n",
        "\n",
        "for x, y, z in MIDDLE_FAILING_TESTCASES:\n",
        "    with ochiai_middle.collect_fail():\n",
        "        middle(x, y, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "253"
        ]
      },
      "outputs": [],
      "source": [
        "ochiai_middle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "254"
        ]
      },
      "source": [
        "We see that the \"culprit\" line is still the most likely to be fixed, but the two conditions leading to the error (`x < y` and `x < z`) are also listed as potentially faulty. That is because the error might also be fixed be changing these conditions \u2013 although this would result in a more complex fix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "255"
        ]
      },
      "source": [
        "## Other Events besides Coverage\n",
        "\n",
        "We close this chapter with two directions for further thought. If you wondered why in the above code, we were mostly talking about `events` rather than lines covered, that is because our framework allows for tracking arbitrary events, not just coverage. In fact, any data item a collector can extract from the execution can be used for correlation analysis. (It may not be so easily visualized, though.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "256"
        ]
      },
      "source": [
        "Here's an example. We define a `ValueCollector` class that collects pairs of (local) variables and their values during execution. Its `events()` method then returns the set of all these pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "257"
        ]
      },
      "outputs": [],
      "source": [
        "class ValueCollector(Collector):\n",
        "    \"\"\"\"A class to collect local variables and their values.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Constructor.\"\"\"\n",
        "        super().__init__()\n",
        "        self.vars: Set[str] = set()\n",
        "\n",
        "    def collect(self, frame: FrameType, event: str, arg: Any) -> None:\n",
        "        local_vars = frame.f_locals\n",
        "        for var in local_vars:\n",
        "            value = local_vars[var]\n",
        "            self.vars.add(f\"{var} = {repr(value)}\")\n",
        "\n",
        "    def events(self) -> Set[str]:\n",
        "        \"\"\"A set of (variable, value) pairs observed\"\"\"\n",
        "        return self.vars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "258"
        ]
      },
      "source": [
        "If we apply this collector on our set of HTML test cases, these are all the events that we obtain \u2013\u00a0essentially all variables and all values ever seen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "259"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ContinuousSpectrumDebugger(ValueCollector))\n",
        "for event in debugger.all_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "260"
        ]
      },
      "source": [
        "However, some of these events only occur in the failing run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "261"
        ]
      },
      "outputs": [],
      "source": [
        "for event in debugger.only_fail_events():\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "262"
        ]
      },
      "source": [
        "Some of these differences are spurious \u2013 the string `\"abc\"` (with quotes) only occurs in the failing run \u2013\u00a0but others, such as `quote` being True and `c` containing a single quote are actually relevant for explaining when the failure comes to be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "263"
        ]
      },
      "source": [
        "We can even visualize the suspiciousness of the individual events, setting the (so far undiscussed) `color` flag for producing an event table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "264"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.event_table(color=True, args=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "265"
        ]
      },
      "source": [
        "There are many ways one can continue from here.\n",
        "\n",
        "* Rather than checking for concrete values, one could check for more _abstract properties_, for instance \u2013 what is the sign of the value? What is the length of the string? \n",
        "* One could check for specifics of the _control flow_ \u2013 is the loop taken? How many times?\n",
        "* One could check for specifics of the _information flow_ \u2013\u00a0which values flow from one variable to another?\n",
        "\n",
        "There are lots of properties that all could be related to failures \u2013\u00a0and if we happen to check for the right one, we may obtain a much crisper definition of what causes the failure. We will come up with more ideas on properties to check as it comes to [mining specifications](SpecificationMining,ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "266"
        ]
      },
      "source": [
        "## Training Classifiers\n",
        "\n",
        "The metrics we have discussed so far are pretty _generic_ \u2013\u00a0that is, they are fixed no matter how the actual event space is structured. The field of _machine learning_ has come up with techniques that learn _classifiers_ from a given set of data \u2013\u00a0classifiers that are trained from labeled data and then can predict labels for new data sets. In our case, the labels are test outcomes (PASS and FAIL), whereas the data would be features of the events observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "267"
        ]
      },
      "source": [
        "A classifier by itself is not immediately useful for debugging (although it could predict whether future inputs will fail or not). Some classifiers, however, have great _diagnostic_ quality; that is, they can _explain_ how their classification comes to be. [Decision trees](https://scikit-learn.org/stable/modules/tree.html) fall into this very category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "268"
        ]
      },
      "source": [
        "A decision tree contains a number of _nodes_, each one associated with a predicate. Depending on whether the predicate is true or false, we follow the given \"true\" or \"false\" branch to end up in the next node, which again contains a predicate. Eventually, we end up in the outcome predicted by the tree. The neat thing is that the node predicates actually give important hints on the circumstances that are _most relevant_ for deciding the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "269"
        ]
      },
      "source": [
        "Let us illustrate this with an example. We build a class `ClassifyingDebugger` that trains a decision tree from the events collected. To this end, we need to set up our input data such that it can be fed into a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "270"
        ]
      },
      "source": [
        "We start with identifying our _samples_ (runs) and the respective _labels_ (outcomes). All values have to be encoded into numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "271"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(DifferenceDebugger):\n",
        "    \"\"\"A debugger implementing a decision tree for events\"\"\"\n",
        "\n",
        "    PASS_VALUE = +1.0\n",
        "    FAIL_VALUE = -1.0\n",
        "\n",
        "    def samples(self) -> Dict[str, float]:\n",
        "        samples = {}\n",
        "        for collector in self.pass_collectors():\n",
        "            samples[collector.id()] = self.PASS_VALUE\n",
        "        for collector in debugger.fail_collectors():\n",
        "            samples[collector.id()] = self.FAIL_VALUE\n",
        "        return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "272"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "debugger.samples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "273"
        ]
      },
      "source": [
        "Next, we identify the _features_, which in our case is the set of lines executed in each sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "274"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def features(self) -> Dict[str, Any]:\n",
        "        features = {}\n",
        "        for collector in debugger.pass_collectors():\n",
        "            features[collector.id()] = collector.events()\n",
        "        for collector in debugger.fail_collectors():\n",
        "            features[collector.id()] = collector.events()\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "275"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "debugger.features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "276"
        ]
      },
      "source": [
        "All our features have names, which must be strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "277"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def feature_names(self) -> List[str]:\n",
        "        return [repr(feature) for feature in self.all_events()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "278"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "debugger.feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "279"
        ]
      },
      "source": [
        "Next, we define the _shape_ for an individual sample, which is a value of +1 or -1 for each feature seen (i.e., +1 if the line was covered, -1 if not)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "280"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def shape(self, sample: str) -> List[float]:\n",
        "        x = []\n",
        "        features = self.features()\n",
        "        for f in self.all_events():\n",
        "            if f in features[sample]:\n",
        "                x += [+1.0]\n",
        "            else:\n",
        "                x += [-1.0]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "281"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "debugger.shape(\"remove_html_markup(s='abc')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "282"
        ]
      },
      "source": [
        "Our input X for the classifier now is a list of such shapes, one for each sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "283"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def X(self) -> List[List[float]]:\n",
        "        X = []\n",
        "        samples = self.samples()\n",
        "        for key in samples:\n",
        "            X += [self.shape(key)]\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "284"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "debugger.X()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "285"
        ]
      },
      "source": [
        "Our input Y for the classifier, in contrast, is the list of labels, again indexed by sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "286"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def Y(self) -> List[float]:\n",
        "        Y = []\n",
        "        samples = self.samples()\n",
        "        for key in samples:\n",
        "            Y += [samples[key]]\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "287"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "debugger.Y()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "288"
        ]
      },
      "source": [
        "We now have all our data ready to be fit into a tree classifier. The method `classifier()` creates and returns the (tree) classifier for the observed runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "289"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "290"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def classifier(self) -> DecisionTreeClassifier:\n",
        "        classifier = DecisionTreeClassifier()\n",
        "        classifier = classifier.fit(self.X(), self.Y())\n",
        "        return classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "291"
        ]
      },
      "source": [
        "We define a special method to show classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "292"
        ]
      },
      "outputs": [],
      "source": [
        "import graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "293"
        ]
      },
      "outputs": [],
      "source": [
        "class ClassifyingDebugger(ClassifyingDebugger):\n",
        "    def show_classifier(self, classifier: DecisionTreeClassifier) -> Any:\n",
        "        dot_data = export_graphviz(classifier, out_file=None, \n",
        "                                   filled=False, rounded=True,\n",
        "                                   feature_names=self.feature_names(),\n",
        "                                   class_names=[\"FAIL\", \"PASS\"],\n",
        "                                   label='none',\n",
        "                                   node_ids=False,\n",
        "                                   impurity=False,\n",
        "                                   proportion=True,\n",
        "                                   special_characters=True)\n",
        "\n",
        "        return graphviz.Source(dot_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "294"
        ]
      },
      "source": [
        "This is the tree we get for our `remove_html_markup()` tests. The top predicate is whether the \"culprit\" line was executed (-1 means no, +1 means yes). If not (-1), the outcome is PASS. Otherwise, the outcome is TRUE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "295"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = test_debugger_html(ClassifyingDebugger())\n",
        "classifier = debugger.classifier()\n",
        "debugger.show_classifier(classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "296"
        ]
      },
      "source": [
        "We can even use our classifier to predict the outcome of additional runs. If, for instance, we execute all lines except for, say, Line 7, 9, and 11, our tree classifier would predict failure \u2013\u00a0because the \"culprit\" line 12 is executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "297"
        ]
      },
      "outputs": [],
      "source": [
        "classifier.predict([[1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "298"
        ]
      },
      "source": [
        "Again, there are many ways to continue from here. Which events should we train the classifier from? How do classifiers compare in their performance and diagnostic quality? There are lots of possibilities left to explore, and we only begin to realize the potential for automated debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "299"
        ]
      },
      "source": [
        "## Synopsis\n",
        "\n",
        "This chapter introduces classes and techniques for _statistical debugging_ \u2013\u00a0that is, correlating specific events, such as lines covered, with passing and failing outcomes.\n",
        "\n",
        "To make use of the code in this chapter, use one of the provided `StatisticalDebugger` subclasses such as `TarantulaDebugger` or `OchiaiDebugger`. \n",
        "\n",
        "Both are instantiated with a `Collector` denoting the type of events you want to correlate outcomes with. The default `CoverageCollector`, collecting line coverage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "300"
        ]
      },
      "source": [
        "### Collecting Events from Calls\n",
        "\n",
        "To collect events from calls that are labeled manually, use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "301"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = TarantulaDebugger()\n",
        "with debugger.collect_pass():\n",
        "    remove_html_markup(\"abc\")\n",
        "with debugger.collect_pass():\n",
        "    remove_html_markup('<b>abc</b>')\n",
        "with debugger.collect_fail():\n",
        "    remove_html_markup('\"abc\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "302"
        ]
      },
      "source": [
        "Within each `with` block, the _first function call_ is collected and tracked for coverage. (Note that _only_ the first call is tracked.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "303"
        ]
      },
      "source": [
        "### Collecting Events from Tests\n",
        "\n",
        "To collect events from _tests_ that use exceptions to indicate failure, use the simpler `with` form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "304"
        ]
      },
      "outputs": [],
      "source": [
        "debugger = TarantulaDebugger()\n",
        "with debugger:\n",
        "    remove_html_markup(\"abc\")\n",
        "with debugger:\n",
        "    remove_html_markup('<b>abc</b>')\n",
        "with debugger:\n",
        "    remove_html_markup('\"abc\"')\n",
        "    assert False  # raise an exception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "305"
        ]
      },
      "source": [
        "`with` blocks that raise an exception will be classified as failing, blocks that do not will be classified as passing. Note that exceptions raised are \"swallowed\" by the debugger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "306"
        ]
      },
      "source": [
        "### Visualizing Events as a Table\n",
        "\n",
        "After collecting events, you can print out the observed events \u2013 in this case, line numbers \u2013\u00a0in a table, showing in which runs they occurred (`X`), and with colors highlighting the suspiciousness of the event. A \"red\" event means that the event predominantly occurs in failing runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "307"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.event_table(args=True, color=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "308"
        ]
      },
      "source": [
        "### Visualizing Suspicious Code\n",
        "\n",
        "If you collected coverage with `CoverageCollector`, you can also visualize the code with similar colors, highlighting suspicious lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "309"
        ]
      },
      "outputs": [],
      "source": [
        "debugger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "310"
        ]
      },
      "source": [
        "### Ranking Events\n",
        "\n",
        "The method `rank()` returns a ranked list of events, starting with the most suspicious. This is useful for automated techniques that need potential defect locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "311"
        ]
      },
      "outputs": [],
      "source": [
        "debugger.rank()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "312"
        ]
      },
      "source": [
        "### Classes and Methods\n",
        "\n",
        "Here are all classes defined in this chapter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "313"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "from ClassDiagram import display_class_hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "314"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "display_class_hierarchy([TarantulaDebugger, OchiaiDebugger],\n",
        "                        abstract_classes=[\n",
        "                            StatisticalDebugger,\n",
        "                            DifferenceDebugger,\n",
        "                            RankingDebugger\n",
        "                        ],\n",
        "                        public_methods=[\n",
        "                            StatisticalDebugger.__init__,\n",
        "                            StatisticalDebugger.all_events,\n",
        "                            StatisticalDebugger.event_table,\n",
        "                            StatisticalDebugger.function,\n",
        "                            StatisticalDebugger.coverage,\n",
        "                            StatisticalDebugger.covered_functions,\n",
        "                            DifferenceDebugger.__enter__,\n",
        "                            DifferenceDebugger.__exit__,\n",
        "                            DifferenceDebugger.all_pass_events,\n",
        "                            DifferenceDebugger.all_fail_events,\n",
        "                            DifferenceDebugger.collect_pass,\n",
        "                            DifferenceDebugger.collect_fail,\n",
        "                            DifferenceDebugger.only_pass_events,\n",
        "                            DifferenceDebugger.only_fail_events,\n",
        "                            SpectrumDebugger.code,\n",
        "                            SpectrumDebugger.__repr__,\n",
        "                            SpectrumDebugger.__str__,\n",
        "                            SpectrumDebugger._repr_html_,\n",
        "                            ContinuousSpectrumDebugger.code,\n",
        "                            ContinuousSpectrumDebugger.__repr__,\n",
        "                            RankingDebugger.rank\n",
        "                        ],\n",
        "                        project='debuggingbook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "315"
        ]
      },
      "outputs": [],
      "source": [
        "# ignore\n",
        "display_class_hierarchy([CoverageCollector, ValueCollector],\n",
        "                        public_methods=[\n",
        "                            Tracer.__init__,\n",
        "                            Tracer.__enter__,\n",
        "                            Tracer.__exit__,\n",
        "                            Tracer.changed_vars,  # type: ignore\n",
        "                            Collector.__init__,\n",
        "                            Collector.__repr__,\n",
        "                            Collector.function,\n",
        "                            Collector.args,\n",
        "                            Collector.argstring,\n",
        "                            Collector.exception,\n",
        "                            Collector.id,\n",
        "                            Collector.collect,\n",
        "                            CoverageCollector.coverage,\n",
        "                            CoverageCollector.covered_functions,\n",
        "                            CoverageCollector.events,\n",
        "                            ValueCollector.__init__,\n",
        "                            ValueCollector.events\n",
        "                        ],\n",
        "                        project='debuggingbook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": true,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "316"
        ]
      },
      "source": [
        "## Lessons Learned\n",
        "\n",
        "* _Correlations_ between execution events and outcomes (pass/fail) can make important hints for debugging\n",
        "* Events occurring only (or mostly) during failing runs can be _highlighted_ and _ranked_ to guide the search\n",
        "* Important hints include whether the _execution of specific code locations_ correlates with failure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "317"
        ]
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Chapters that build on this one include\n",
        "\n",
        "* [how to determine invariants that correlate with failures](DynamicInvariants.ipynb)\n",
        "* [how to automatically repair programs](Repairer.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "318"
        ]
      },
      "source": [
        "## Background\n",
        "\n",
        "The seminal works on statistical debugging are two papers:\n",
        "\n",
        "* \"Visualization of Test Information to Assist Fault Localization\" \\cite{Jones2002} by James Jones, Mary Jean Harrold, and John Stasko introducing Tarantula and its visualization. The paper won an ACM SIGSOFT 10-year impact award.\n",
        "* \"Bug Isolation via Remote Program Sampling\" \\cite{Liblit2003} by Ben Liblit, Alex Aiken, Alice X. Zheng, and Michael I. Jordan, introducing the term \"Statistical debugging\". Liblit won the ACM Doctoral Dissertation Award for this work.\n",
        "\n",
        "The Ochiai metric for fault localization was introduced by \\cite{Abreu2009}. The overview by Wong et al. \\cite{Wong2016} gives a comprehensive overview on the field of statistical fault localization.\n",
        "\n",
        "The study by Parnin and Orso \\cite{Parnin2011} is a must to understand the limitations of the technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": true,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "319"
        ]
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "solution2": "hidden",
        "solution2_first": true,
        "tags": [
          "320"
        ]
      },
      "source": [
        "### Exercise 1: A Postcondition for Middle\n",
        "\n",
        "What would be a postcondition for `middle()`? How can you check it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "321"
        ]
      },
      "source": [
        "**Solution.** A simple postcondition for `middle()` would be\n",
        "\n",
        "```python\n",
        "assert m == sorted([x, y, z])[1]\n",
        "```\n",
        "\n",
        "where `m` is the value returned by `middle()`. `sorted()` sorts the given list, and the index `[1]` returns, well, the middle element. (This might also be a much shorter, but possibly slightly more expensive implementation for `middle()`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "322"
        ]
      },
      "source": [
        "Since `middle()` has several `return` statements, the easiest way to check the result is to create a wrapper around `middle()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "323"
        ]
      },
      "outputs": [],
      "source": [
        "def middle_checked(x, y, z):  # type: ignore\n",
        "    m = middle(x, y, z)\n",
        "    assert m == sorted([x, y, z])[1]\n",
        "    return m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "324"
        ]
      },
      "source": [
        "`middle_checked()` catches the error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "325"
        ]
      },
      "outputs": [],
      "source": [
        "from ExpectError import ExpectError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "326"
        ]
      },
      "outputs": [],
      "source": [
        "with ExpectError():\n",
        "    m = middle_checked(2, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "tags": [
          "327"
        ]
      },
      "source": [
        "### Exercise 2: Statistical Dependencies\n",
        "\n",
        "Using the dependencies from [the chapter on slicing](Slicer.ipynb), can you determine which specific data or control dependencies correlate with failure?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "328"
        ]
      },
      "source": [
        "### Exercise 3: Correlating with Conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "329"
        ]
      },
      "source": [
        "In HTML, it is permissible that tag attribute values can also have single quotes, as in\n",
        "\n",
        "```html\n",
        "<b class='extrabold'>abc</b>\n",
        "```\n",
        "\n",
        "Such attributes are actually treated correctly by our `remove_html_markup()` code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "solution2": "hidden",
        "solution2_first": true,
        "tags": [
          "330"
        ]
      },
      "source": [
        "#### Part 1: Experiment\n",
        "\n",
        "What happens if test inputs with single quote attributes become part of our test set? How does statistical fault localization change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "solution2": "hidden",
        "tags": [
          "331"
        ]
      },
      "source": [
        "**Solution.** The effect is that the line `quote = not quote` will be executed both in passing and failing runs, spoiling the correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "332"
        ]
      },
      "source": [
        "#### Part 2: Collecting Conditions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "333"
        ]
      },
      "source": [
        "Instead of aiming for _lines_ that correlate with failures, we can look at individual _branch conditions_ such as `c == \"'\"`, `c == '\"'`, or `tag` that correlate with failures. In the above case, the condition `c == \"'\"` would correlate, whereas `c == '\"'` would not. \n",
        "\n",
        "Reusing the code instrumentation from [the chapter on slicing](Slicer.ipynb), collect the individual values of Boolean conditions in tests during execution in a `ConditionCollector` class. Show the event table."
      ]
    }
  ],
  "metadata": {
    "ipub": {
      "bibliography": "fuzzingbook.bib",
      "toc": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "toc-autonumbering": false
  },
  "nbformat": 4,
  "nbformat_minor": 4
}